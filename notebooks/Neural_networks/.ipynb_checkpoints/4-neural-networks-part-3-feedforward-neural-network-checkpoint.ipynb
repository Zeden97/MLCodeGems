{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read this blog first: [Blog](https://rakesh-malviya.github.io/blog/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Define activations \n",
    "\n",
    "###### Sigmoid\n",
    "Sigmoid function, $$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "derivative of sigmoid, $$\\frac{d}{dx} f(x) = f(x) (1 - f(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        def sig(x):\n",
    "            return 1 / (1 + math.exp(-x))\n",
    "\n",
    "        def sig_grad(x):\n",
    "            sigx = sig(x)\n",
    "            return sigx*(1-sigx)\n",
    "\n",
    "        self.func_vec = np.vectorize(sig)\n",
    "        self.func_vec_grad = np.vectorize(sig_grad)\n",
    "\n",
    "\n",
    "    def eval(self,z):\n",
    "        return self.func_vec(z)\n",
    "\n",
    "    def eval_grad(self,z):\n",
    "        return self.func_vec_grad(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tanh\n",
    "Tanh function, $$f(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \\frac{sinh(x)}{cosh(x)}$$\n",
    "derivative of Tanh, $$\\frac{d}{dx} f(x) = 1 - (f(x))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return math.tanh(x)\n",
    "\n",
    "        def tanh_grad(x):\n",
    "            tanhx = tanh(x)\n",
    "            return (1- tanhx*tanhx)\n",
    "\n",
    "        self.func_vec = np.vectorize(tanh)\n",
    "        self.func_vec_grad = np.vectorize(tanh_grad)\n",
    "\n",
    "    def eval(self, z):\n",
    "        return self.func_vec(z)\n",
    "\n",
    "    def eval_grad(self, z):\n",
    "        return self.func_vec_grad(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,layer_size,input_size,act_func=None):\n",
    "        self.layer_size = layer_size\n",
    "        self.input_size = input_size\n",
    "        self.W = np.random.normal(loc=0,scale=1.0,size=(self.layer_size,self.input_size))\n",
    "        self.b = np.zeros(shape=(self.layer_size,1)) + 0.1\n",
    "        self.h = None #output\n",
    "        self.z = None\n",
    "        self.grad_act_z = None\n",
    "        self.delta = None\n",
    "        self.grad_w = np.zeros(shape=self.W.shape)\n",
    "        self.grad_b = np.zeros(shape=self.b.shape)\n",
    "        self.grad_w_count = 0\n",
    "        self.grad_b_count = 0\n",
    "\n",
    "\n",
    "        if act_func!=None:\n",
    "            self.act_func = act_func\n",
    "        else:\n",
    "            self.act_func = Tanh()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{z}^l = \\mathbf{W}^l \\otimes \\mathbf{h}^{l-1} + \\mathbf{b}^l$\n",
    "where, $\\mathbf{h}^{l-1}$ is input_h in code\n",
    "\n",
    "grad_act_z is, $\\sigma'({\\mathbf{z}^l})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def eval_z(self,input_h):\n",
    "\n",
    "        #Test input shape\n",
    "        if input_h.shape[0]!=self.input_size or input_h.shape[1]!=1:\n",
    "            raise ValueError(\"input shape : %s expected (%d,1)\"\n",
    "                             %(str(input_h.shape),self.input_size))\n",
    "\n",
    "        self.z = np.dot(self.W,input_h) + self.b\n",
    "\n",
    "        # Test z shape\n",
    "        if self.z.shape[0] != self.layer_size or self.z.shape[1] != 1:\n",
    "            raise ValueError(\"input shape : %s expected (%d,1)\"\n",
    "                             % (str(self.z.shape), self.layer_size))\n",
    "\n",
    "        self.grad_act_z = self.act_func.eval_grad(self.z)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{h}^l = \\sigma(\\mathbf{z}^l)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def eval_h(self,input_h):\n",
    "        self.eval_z(input_h)\n",
    "        self.h = self.act_func.eval(self.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pmb{\\delta}^l = ((\\mathbf{W}^{l+1})^T \\otimes \\pmb{\\delta}^{l+1} )\\odot  \\sigma'({\\mathbf{z}^l})$, \n",
    "\n",
    "where in code\n",
    "1. $\\mathbf{W}^{l+1}$ is self.W\n",
    "2. $\\pmb{\\delta}^{l+1}$ is self.dalta\n",
    "3. $\\sigma'({\\mathbf{z}^l})$ is grad_act_z_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def set_delta(self,delta):\n",
    "        self.delta = delta\n",
    "\n",
    "    \"\"\"\n",
    "    delta^(l-1) = [ (W^l)^T delta^l ] grad(act(z^(l-1)))\n",
    "    \"\"\"\n",
    "    def eval_delta_back(self,grad_act_z_back):\n",
    "        if self.delta is None:\n",
    "            raise ValueError(\"self.delta is None\")\n",
    "\n",
    "        if self.grad_act_z is None:\n",
    "            raise ValueError(\"self.grad_act_z is None\")\n",
    "\n",
    "        return np.matul(np.transpose(self.W),self.delta) * grad_act_z_back\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{\\mathbf{b}^l}J =  \\pmb{\\delta}^l$ \n",
    "\n",
    "$\\nabla_{\\mathbf{W}^l}J =  \\pmb{\\delta}^l \\otimes (\\mathbf{h}^{l-1})^T$\n",
    "\n",
    "$\\mathbf{b}_l = \\mathbf{b}_l -\\eta \\cdot \\frac{1}{m} \\cdot \\sum_{i=1}^{m} \\nabla_{\\mathbf{b}^l}J^{(i)} $\n",
    "\n",
    "$\\mathbf{W}_l = \\mathbf{W}_l - \\eta \\cdot \\frac{1}{m} \\cdot \\sum_{i=1}^{m} \\nabla_{\\mathbf{W}^l}J^{(i)} $\n",
    "\n",
    "where in code\n",
    "1. $\\nabla_{\\mathbf{b}^l}J$ is self.grad_w\n",
    "2. $\\nabla_{\\mathbf{b}^l}J$ is self.grad_b\n",
    "3. $\\pmb{\\delta}^l$ i self.delta\n",
    "4. $\\mathbf{h}^{l-1}$ i input_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def accum_grad(self,input_h):\n",
    "        self.grad_w += np.matmul(self.delta, np.transpose(input_h))\n",
    "        self.grad_b += self.delta\n",
    "        self.grad_w_count += 1\n",
    "        self.grad_b_count += 1\n",
    "        \n",
    "    def update_w(self,lr):\n",
    "        self.W = self.W - self.grad_w*lr/self.grad_w_count\n",
    "        self.b = self.b - self.grad_b*lr/self.grad_b_count\n",
    "        self.grad_w = np.zeros(shape=self.W.shape)\n",
    "        self.grad_b = np.zeros(shape=self.b.shape)\n",
    "        self.grad_w_count = 0\n",
    "        self.grad_b_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,input_size,layers,lr=0.01,batch_size=10):\n",
    "        self.input_size = input_size\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        self.train_step_count = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        cur_input_size = self.input_size\n",
    "        for cur_layer_size in layers[:-1]:\n",
    "            self.layers.append(Dense(cur_layer_size,cur_input_size))\n",
    "            cur_input_size = cur_layer_size\n",
    "\n",
    "\n",
    "        self.layers.append(Dense(layers[-1],cur_input_size,act_func=Sigmoid()))\n",
    "\n",
    "        print([x.layer_size for x in self.layers])\n",
    "\n",
    "\n",
    "    def forward_step(self,input_x):\n",
    "        cur_input_h = input_x\n",
    "        # print(self.layers[1].W)\n",
    "        for layer in self.layers:\n",
    "            layer.eval_h(cur_input_h)\n",
    "            cur_input_h = layer.h\n",
    "\n",
    "        return cur_input_h\n",
    "\n",
    "    def backprop_step(self,input_x):\n",
    "        rev_layer_idxes = list(range(len(self.layers)))[:-1]\n",
    "        rev_layer_idxes.reverse()\n",
    "        # print(rev_layer_idxes)\n",
    "        # backpropagation\n",
    "        for layer_idx in rev_layer_idxes:\n",
    "            prev_layer = self.layers[layer_idx+1]\n",
    "            layer = self.layers[layer_idx]\n",
    "            prev_layer.accum_grad(layer.h)\n",
    "            cur_delta = prev_layer.eval_delta_back(layer.grad_act_z)\n",
    "            layer.set_delta(cur_delta)\n",
    "\n",
    "        layer.accum_grad(input_x)\n",
    "\n",
    "    def train_step(self,input_x,y):\n",
    "        #forward step\n",
    "        pred_y = self.forward_step(input_x=input_x)\n",
    "\n",
    "        #Cost grading\n",
    "        y = np.array(y)\n",
    "        y = y.reshape((-1,1))\n",
    "        cur_delta = (pred_y-y)*self.layers[-1].grad_act_z\n",
    "        self.layers[-1].set_delta(cur_delta)\n",
    "\n",
    "        #backpropagation step\n",
    "        self.backprop_step(input_x)\n",
    "        self.train_step_count+=1\n",
    "        if self.train_step_count%self.batch_size==0:\n",
    "            self.updateW()\n",
    "\n",
    "\n",
    "    def updateW(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update_w(self.lr)\n",
    "\n",
    "    def predict(self,X_train,y_train):\n",
    "        pred_y = []\n",
    "        pred_pro_y = []\n",
    "        for j in range(X_train.shape[0]):\n",
    "            X = X_train[j, :]\n",
    "            X = X.reshape((-1, 1))\n",
    "            y = y_train[j]\n",
    "            y = y.reshape((-1, 1))\n",
    "            y_p = self.forward_step(X)\n",
    "            pred_pro_y.append(y_p[0])\n",
    "            if y_p[0] > 0.5:\n",
    "                pred_y.append(1)\n",
    "            else:\n",
    "                pred_y.append(0)\n",
    "\n",
    "        pred_y = np.array(pred_y)\n",
    "        pred_pro_y = np.array(pred_pro_y)\n",
    "\n",
    "        return pred_y,pred_pro_y\n",
    "\n",
    "\n",
    "    def train(self,epoch,X_train,y_train,X_test=None,y_test=None,\n",
    "              monitor_train_acc=False,monitor_test_acc=False,monitor_checkpoint=10):\n",
    "        for iepo in range(epoch):\n",
    "            for j in range(X_train.shape[0]):\n",
    "                X = X_train[j, :]\n",
    "                X = X.reshape((-1, 1))\n",
    "                y = y_train[j]\n",
    "                y = y.reshape((-1, 1))\n",
    "                self.train_step(X, y)\n",
    "\n",
    "\n",
    "            if (iepo % monitor_checkpoint == 0):\n",
    "                if monitor_train_acc and X_train is not None and y_train is not None:\n",
    "                    pred_y, pred_pro_y = self.predict(X_train,y_train)\n",
    "                    print(\"%d Train acc:%f mse:%f lr:%f\" % (\n",
    "                    iepo, accuracy_score(y_train, pred_y), mean_squared_error(y_train, pred_pro_y), self.lr))\n",
    "\n",
    "                if monitor_test_acc and X_test is not None and y_test is not None:\n",
    "                    pred_y, pred_pro_y = self.predict(X_test, y_test)\n",
    "                    print(\"%d Test acc:%f mse:%f lr:%f\" % (\n",
    "                    iepo, accuracy_score(y_test, pred_y), mean_squared_error(y_test, pred_pro_y), self.lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to Generate two-spiral dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "From tensorflow playground code\n",
    "\"\"\"\n",
    "\n",
    "def gen2Spiral(sample=500,noise=0,addSin=False):\n",
    "    n = sample/2\n",
    "    Xdata = []\n",
    "    Ydata = []\n",
    "\n",
    "    def genSpiral(deltaT,label):\n",
    "        for i in range(n):\n",
    "            r = float(i)/n*5.0\n",
    "            t = 1.75*float(i)/n*2.0*math.pi + deltaT\n",
    "            x = r*math.sin(t) #+ np.random.uniform(-1,1)*noise\n",
    "            y = r * math.cos(t)  # + np.random.uniform(-1,1)*noise\n",
    "\n",
    "\n",
    "            if addSin:\n",
    "                Xdata.append([x, y,math.sin(x),math.sin(y)])\n",
    "            else:\n",
    "                Xdata.append([x, y])\n",
    "\n",
    "            Ydata.append(label)\n",
    "\n",
    "\n",
    "    genSpiral(0,1.0)\n",
    "    genSpiral(math.pi,0.0)\n",
    "\n",
    "    Xdata = np.array(Xdata)\n",
    "    Ydata = np.array(Ydata)\n",
    "    return Xdata,Ydata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Code to call the above code\"\"\" \n",
    "\n",
    "Xdata,Ydata = gen2Spiral(addSin=True)\n",
    "nn = NeuralNetwork(4,[4,2,1],lr=0.05)\n",
    "# Xdata,Ydata = spiralgen.gen2Spiral()\n",
    "# nn = NeuralNetwork(2,[8,8,5,1],lr=0.003)\n",
    "epoch = 10000\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xdata, Ydata, test_size=0.30,random_state=1010)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "nn.train(epoch,X_train,y_train,X_test=X_test,y_test=y_test,\n",
    "         monitor_train_acc=True,monitor_test_acc=True,monitor_checkpoint=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
